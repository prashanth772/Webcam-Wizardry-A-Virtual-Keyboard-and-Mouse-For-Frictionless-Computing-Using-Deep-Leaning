# Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning

The aim of this project is to develop a system that uses a webcam to track the movement of a virtual mouse cursor and finger air writing on a computer screen. It can be used as an assistive technology for people with limited mobility or disabilities, allowing them to control their computers using hand gestures. A virtual mouse cursor is a software-generated cursor that simulates the behaviour of a physical mouse cursor. Some of its features include position tracking, click and drag, volume up, volume down using two or more fingers. The virtual mouse cursor can be implemented with the following steps - Capturing video frames from a camera or webcam, Preprocessing the frames to remove noise and enhance the hand contour, Detecting and isolating the hand from the background, Tracking the hand and finger movements, Mapping the hand and finger movements to cursor movements on the computer screen. The finger air writing method is a way of inputting text, allowing users to write in the air without the need for a physical keyboard. The system captures video of the user's hand and finger movements using a camera or webcam and processes the video using OpenCV to detect and track the hand and fingers. It can be implemented by following steps - Capturing video frames from a camera, preprocessing the frames to remove noise and enhance the hand and finger contours, detecting and isolating the hand and fingers from the background, tracking the hand and finger movements, recognizing the hand gestures as characters or letters, displaying the recognized text on a computer screen. We recognize the hand gestures using machine learning algorithms like Convolutional Neural Networks (CNN) and Markov Random Field Convolutional Neural Network (MRF-CNN) and the output is stored in a text fThe advancement of technology has brought about significant improvements in the field of assistive technologies, particularly for individuals with limited mobility or disabilities. In this project, our aim is to develop a system that utilises a webcam to track the movement of a virtual mouse cursor and enable finger air writing on a computer screen.[1] This system, known as the Hand Gesture Mouse Interface (HGMI), captures hand gestures from the user through a web camera and uses the primitive hand movements for the respective gesture. This innovative solution allows users to control their computers through intuitive hand gestures, eliminating the need for a physical mouse and keyboard.

The core functionality of our system revolves around the implementation of a virtual mouse cursor and the recognition of finger air writing. The virtual mouse cursor simulates the behaviour of a physical mouse cursor, providing features such as position tracking, click and drag, and multi-touch support, including zooming and scrolling using multiple fingers. By capturing video frames from a camera or webcam, preprocessing these frames to remove noise and enhance the hand contour, detecting and isolating the hand from the background, tracking hand and finger movements, and mapping these movements to cursor movements on the computer screen, we create a seamless user experience.

Additionally, the finger air writing method enables users to input text without the need for a physical keyboard. By capturing video of the user's hand and finger movements and processing it using OpenCV, we can detect and track the hand and fingers, recognize hand gestures as characters or letters, and display the recognized text on the computer screen. This provides an alternative and accessible means of inputting text, enhancing the usability of the system for individuals with limited dexterity.

To achieve accurate gesture recognition, we leverage various machine learning algorithms such as Random Forest Classifier, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Hidden Markov Models (HMMs), and K-Nearest Neighbors (KNN). Employing an ensemble process, we determine the most effective and precise algorithm for gesture recognition. The recognized output is initially stored in a text file, and upon completion of the air writing process, the software prompts the user to choose between speech or saving the text in formats such as .txt, .csv, or .docx. This flexibility enables users to seamlessly integrate their input with other applications and tools. By eliminating the physical barriers imposed by traditional input devices, our system offers a novel and inclusive approach to computer control, enhancing the overall user experience and enabling greater independence.

In the following documentation, we will delve deeper into the technical aspects of the project, including the implementation steps, the machine learning algorithms utilised, and the user interface design. We will also provide comprehensive instructions for users to set up and utilise the system effectively. Let us embark on this journey of innovation and accessibility, creating a world where everyone can engage with technology with ease.
Throughout this documentation, we will delve into the technical aspects of the project, including the implementation steps, machine learning algorithms employed, and user interface design. We will also provide comprehensive instructions for users to set up and utilise the system effectively.
ile initially, after finishing the air writing process the software prompts the user to choose to speak or save the text in a particular format(.txt, .csv, .docx) and based on the choice of the user the software responds correspondingly. Finally the goal is to create an intuitive and user-friendly interface that allows users to control their computer without the need for a physical mouse and a keyboard.


![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/ad33baf8-a50f-4ac3-8d81-3bd203cc1c7d)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/03b94a0d-81a2-4492-8e62-e17bf47f04dc)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/d200e329-6a66-4b89-b80c-4a5660559310)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/bc0fee01-4a6b-4128-a7a6-34d0c4426736)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/7fa37211-d6a0-44bb-9174-8afef7a01c25)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/df12e5fc-b3bf-4c03-abb2-bd3d34cb0550)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/8e5f8f9a-b322-4362-acc0-cbd31024a7dc)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/4a44ee8a-2417-4d2d-8872-8114eb1392a6)

![image](https://github.com/prashanth772/Webcam-Wizardry-A-Virtual-Keyboard-and-Mouse-For-Frictionless-Computing-Using-Deep-Leaning/assets/124067333/4c93c2cd-2694-4729-98c6-0d13bcda6bea)
